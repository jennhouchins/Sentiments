---
  title: "Academic Mental Health on Twitter"
  subtitle: "**<small>Sentiment Analysis in R</small>**"
  author: "Jennifer Houchins"
  date: '`r format(Sys.time(), "%B %d, %Y")`'
  output:
    xaringan::moon_reader:
      css: [default, metropolis, metropolis-fonts]    
      lib_dir: libs                        # creates directory for libraries
      chakra: libs/remark-latest.min.js
      yolo:
        img: img/yolo-cat.jpeg
        times: 2
      seal: true                          # false: custom title slide
      nature:
        highlightStyle: googlecode         # highlighting syntax for code
        highlightLines: true               # true: enables code line highlighting 
        highlightLanguage: ["r"]           # languages to highlight
        countIncrementalSlides: false      # false: disables counting of incremental slides
        ratio: "4:3"                      # 4:3 for standard size,16:9
   # includes:
     # after_body: [css/insert-logo.html] # adds NHS logo to slides (you can ignore it)
---
class: inverse, center, middle
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
```{r, echo=FALSE}
# then load all the relevant packages
pacman::p_load(pacman, tidyverse, tidytext, wordcloud2, readxl, 
               forcats, remotes, ggplot2, rtweet, htmlwidgets, 
               vader, webshot, ggcats, Hmisc)
```

```{r xaringan-panelset, echo=FALSE}
xaringanExtra::use_panelset()
```
.font130[I have seen multiple news articles about **toxic academic/research environments** leading to **graduate student suicides**.] 
.font125[<br/><br/>For example, see this InsideHigherEd article:<br/> <https://www.insidehighered.com/news/2021/03/03/u-florida-suspends-professor-blamed-students-suicide?mc_cid=96abfba41b&mc_eid=152b28524a>]


.font130[I have also been seeing **mental health related tweets** in the Twitter community of **AcademicChatter**. So, I decided to investigate these tweets for my sentiment analysis...]

???

To give you a bit of context for why I chose to do this particular sentiment analysis... I've been seeing reports (mostly via sites like InsideHigherEd) that suggest that academia is producing some very toxic environments with some very detrimental results.

In particular, I've seen multiple reports of faculty being investigated after reports that they created a toxic research environment that resulted in graduate student suicides. One of these was just posted last week when I started this analysis! I've shared it here.

Additionally, I follow a social media community called AcademicChatter which is an academia-focused Twitter account and I quite frequently notice tweets or retweets that mention mental health, either in the text or by hashtags. So this was the impetus for my choice of topic for this week's independent sentiment analysis assignment.

---

#GUIDING QUESTIONS
<br/>
???

I had two guiding questions for this independent analysis
--
<br/>
## 1. What sentiments are most prevalent in academic mental health related tweets?

???

The first was 

What sentiments are most prevalent in academic mental health related tweets?

--
<br/>
## 2. How do sentiment analysis results for these tweets differ by lexicon?

???

and the second was

- How do these results differ across lexicons?

---
# SEARCHING TWITTER

.font95[**To search for relevant tweets, I set up a dictionary of search terms that used conditional logic (e.g., AND, OR) with various combinations of mental health and AcademicChatter/AcademicTwitter. It looked like this:**]
<br/>
<br/>
```{r dictionary, echo=TRUE}
ac_dictionary <- c("#mentalhealth AND #AcademicChatter OR academicchatter",
                   "#mentalhealth AND #AcademicTwitter OR academictwitter",
                   "#mentalhealth AND #PhD OR #phdlife OR #phdchat",
                   '"mental health" AND #AcademicChatter',
                   '"mental health" AND #AcademicTwitter',
                   '"mental health" AND @AcademicChatter',
                   '"mental health" AND "phd students"',
                   "#AcademicMentalHealth OR academicmentalhealth")

```

???

The first thing I needed to do was get some tweets to analyze so I built a dictionary various combinations of hashtags, search terms, and users to search Twitter. These focused on mental health and either AcademicChatter or Academic Twitter. Many of the hashtags I used are based on those I see frequently used in AcademicChatter tweets. AcademicChatter is an account that I follow on Twitter.

---
# SENTIMENT ANALYSIS
<br/>
### For my analysis, I used two lexicons:
<br/>

* **The NRC Emotion lexicon** (<https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm>)<br/><br/>

* **The Valence Aware Dictionary and sEntiment Reasoner (VADER)
 lexicon** (<https://pypi.org/project/vaderSentiment/>)


???

For this sentiment analysis project, I used two lexicons. The first is NRC.

You're probably familiar with NRC from our assigned readings and Unit 2 Walkthrough. It is a list of English words with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive)

The second is the Valence Aware Dictionary and sEntiment Reasoner lexicon or VADER. Soraya shared this lexicon in our discussion forums. It's a lexicon and rule-based sentiment analysis tool developed by Hutto & Gilbert in 2014 that is specifically attuned to sentiments expressed in social media. It also claims to work well on texts from other domains. Therefore, it seemed like an appropriate choice to use to analyze tweets.

I also thought this would make for an interesting comparison. While NRC focuses on unigrams, the VADER package gives sentiment scores (positive, negative, and neutral) for the text (of the tweet) as a whole.

---
# ANALYSING THE TWEETS
.panelset[
.panel[.panel-name[TWEETS]
**I read the saved tweets from my Excel data file and tokenized the text for analysis.**

```{r import-data, cache=TRUE, results='hide', echo=TRUE}
datafilePath <- "data/academicmentalhealth_tweets.xlsx"
mh_tweets <- read_xlsx(datafilePath)

chatter_text <-
  mh_tweets %>%
  filter(lang == "en") %>%
  select(screen_name, created_at, text) 

tweet_tokens <- chatter_text %>%
  unnest_tokens(output = word, 
                input = text, 
                token = "tweets") %>% 
  anti_join(stop_words, by = "word")
```
]
.panel[.panel-name[NRC]
**Then I processed the tweets to get the emotions and sentiments using the NRC lexicon.**
```{r nrc-chunk, results='hide', echo=TRUE}
nrc <- get_sentiments("nrc")
sentiment_nrc <- inner_join(tweet_tokens, nrc, by = "word")

summary_nrc <- sentiment_nrc %>% 
  count(sentiment, sort = TRUE) %>% 
  spread(sentiment, n) %>%
  mutate(lexicon = "nrc") %>%
  relocate(lexicon)
```
]

.panel[.panel-name[VADER]
**Then I processed the tweets for sentiment using the VADER lexicon. This uses much less code because we don't have to tokenize the tweets first!! **

```{r vader-chunk, results='markup', echo=TRUE, cache=TRUE}
summary_vader <- vader_df(chatter_text$text) %>% 
  select(text, compound, pos, neg, neu)
```
```{r vader-ex, echo=FALSE}
knitr::kable(head(summary_vader,1), format="html", align = "ccccc")
```
]
]

---

# THE RESULTS
.panelset[
.panel[.panel-name[NRC]
**NRC results suggest that the tweets for academic mental health are positive overall. This seems to jive with the highest expressed emotion which is trust.**
<br/>
<br/>
```{r nrc-sents, echo=FALSE}
tweet_sents <- summary_nrc %>% 
  select(positive, negative)

knitr::kable(tweet_sents, format="html", align = "cc")
```
<br/>
<br/>

```{r nrc-emots, echo=FALSE}
tweet_emots <- summary_nrc %>% 
  select(anger, anticipation, disgust, fear, joy, sadness, surprise, trust)

knitr::kable(tweet_emots, format="html", align = "cccccccc")
```
]

.panel[.panel-name[VADER]
**We have to do a little processing to get comparable results from VADER. VADER assigns positive, negative, and neutral scores to each tweet and then normalizes those to provide a compound score. This is what we'll look at.**

```{r vader-results, echo=TRUE}

vader_results <- summary_vader %>% 
  mutate(sentiment = ifelse(compound > 0, "positive", 
                            ifelse(compound < 0, "negative", "neutral"))) %>% 
  count(sentiment, sort = TRUE) %>% 
  spread(sentiment, n) %>% 
  relocate(positive)
```
```{r display-vader, echo=FALSE}
knitr::kable(vader_results, format="html", align = "ccc")
```
]

.panel[.panel-name[COMPARISON]
<br/>
.center[**Let's look at a side-by-side comparison of the sentiments.**]
<br/>
<br/>
.pull-left[

```{r table-nrc, echo=FALSE}
knitr::kable(tweet_sents, format="html", align = "cc", caption = "NRC Sentiments")
```

]
.pull-right[

```{r table-vader, echo=FALSE}
knitr::kable(vader_results, format="html", align = "ccc", caption = "VADER Sentiments")
```

]
]
]

???
NRC results suggest that the tweets for academic mental health are more positive than negative and the most frequently expressed emotion is trust.

So, let's look at VADER. Recalling that VADER gives a score for the tweet as whole rather than using unigrams, we know that the results produced are very different... So, I created a sentiment column in my summary_vader dataframe which contains the value positive if the compound score is greater than 0, negative if it's less than 0, and neutral otherwise. Then we'll count the sentiment to get results that are comparable to those using the NRC lexicon.

---
# POSITIVE TWEETS

**Using the VADER results, we can see a sampling of the most positive tweets...**
<br/>
.panelset[
.panel[.panel-name[R Code]
```{r most-pos, echo=TRUE, cache=TRUE}

positiveTweet <- summary_vader %>% 
  filter(compound > 0.75) %>% 
  mutate(tweet = text) %>% 
  sample_n(3) %>%
  select(tweet)

```
]
.panel[.panel-name[Results]
```{r tweets, echo=FALSE}
knitr::kable(positiveTweet, format="html", align = "c")
```
]
]

---

# REFLECTING ON THE RESULTS
## I had two concerns when I began this analysis...
<br/>
* .font150[Potential differences in results based on lexicons.]


???
Initially, I was concerned about the potential differences in the overall results based on how differently these lexicons treat the sentiment analysis. It would seem like NRC could mis-classify sentiment because it processes things as unigrams but these results suggest that isn't the case.

--

* .font150[Expectation of mostly negative conversations about academic mental health.]

???

I really did expect the tweets regarding academic mental health to be less positive, but it seems that the AcademicChatter Twitter community has fostered a measure of trust (as indicated by the NRC lexion's results). This may suggest that the tweets are indicative of a supportive community rather than a sounding board for complaints.

--
<br/>
<br/>
<br/>
<br/>
.center[.font150[**Neither of these turned out to be the case!**]]

???

I was pleasantly surprised that not only did the two lexicons produce consistent results despite the differences in the way they work, but also that the tweets were positive overall.

---

class: middle, inverse

.pull-left[
<br/>
# **Thank you!**
<br/>
If you'd like to learn more about the techniques used in this
presentation you can get the R project from my
Github repo:
<https://github.com/jennhouchins/Sentiments.git>
<br/>
<br/>
<br/>
**Follow me on Twitter: @TooSweetGeek** 
]

.pull-right[
.center[
<img style="border-radius: 50%;" src="img/jenn.jpg" width="225px"/>

]
]

???

Thanks for listening to my presentation. This presentation uses the R markdown Xaringan presentation package. One of the things I like about this package is the moon_reader addin for R Studio that let me see the slides update in the viewer pane as I was working. This is something that I couldn't do with the Reveal.js package and it was kind of nice to see the slides update real-time rather than having to constantly knit to see my changes. The thing I missed from that package was the ease of animating slide transitions, but there are always trade-offs.